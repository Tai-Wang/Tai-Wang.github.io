<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tai-wang.github.io/">Tai Wang</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=-zT1NKwAAAAJ&hl=zh-CN">Xiaohan Mao</a><sup>1,2*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=QabwS_wAAAAJ&hl=zh-CN">Chenming Zhu</a><sup>1,3*</sup>,
            </span>
            <span class="author-block">
              <a href="https://runsenxu.com/">Runsen Xu</a><sup>1,4</sup>,
            </span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Ruiyuan_Lyu1">Ruiyuan Lyu</a><sup>1,5</sup>,
            </span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Peisen_Li1">Peisen Li</a><sup>1,5</sup>,
            </span>
            <span class="author-block">
              <a href="https://xiao-chen.info/">Xiao Chen</a><sup>1,4</sup>,
            </span>
            <span class="author-block">
              <a href="http://zhangwenwei.cn/">Wenwei Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://chenkai.site/">Kai Chen</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://tianfan.info/">Tianfan Xue</a><sup>1,4</sup>,
            </span>
            <span class="author-block">
              <a href="https://xh-liu.github.io/">Xihui Liu</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.mvig.org/">Cewu Lu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://dahua.site/">Dahua Lin</a><sup>1,4</sup>,
            </span>
            <span class="author-block">
              <a href="https://oceanpang.github.io/">Jiangmiao Pang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai AI Laboratory,</span>
            <span class="author-block"><sup>2</sup>Shanghai Jiao Tong University,</span>
            <span class="author-block"><sup>3</sup>The University of Hong Kong,</span>
            <span class="author-block"><sup>4</sup>The Chinese University of Hong Kong,</span>
            <span class="author-block"><sup>5</sup>Tsinghua University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/paper/EmbodiedScan.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- TBD!!!. -->
              <span class="link-block">
                <a href="https://tai-wang.github.io/embodiedscan/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--               <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/OpenRobotLab/EmbodiedScan" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/OpenRobotLab/EmbodiedScan" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser"               
               controls
               muted
               preload
               playsinline height="100%">
        <source src="./static/videos/demo_video.mp4"
                type="video/mp4">
      </video>
    </div>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">EmbodiedScan</span> provides a multi-modal, ego-centric 3D perception dataset and benchmark for holistic 3D scene understanding.
        Building upon this database, our baseline framework, Embodied Perceptron could process an arbitrary number of multi-modal inputs and demonstrates remarkable 3D perception capabilities, both within the two series of benchmarks we set up and in the wild.
        </h2>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          In the realm of computer vision and robotics, embodied agents are expected to explore their environment and carry out human instructions. 
          This necessitates the ability to fully understand <b>3D scenes</b> given their <b>first-person observations</b> and contextualize them into language for interaction.
          However, traditional research focuses more on scene-level input and output setups from a global view.
          To address the gap, we introduce <b>EmbodiedScan, a multi-modal, ego-centric 3D perception dataset and benchmark for holistic 3D scene understanding.</b> 
          It encompasses over <b>5k scans encapsulating 1M ego-centric RGB-D views, 1M language prompts, 160k 3D-oriented boxes spanning over 760 categories, some of which partially align with LVIS, and dense semantic occupancy with 80 common categories.</b> 
          Building upon this database, we introduce a baseline framework named <b>Embodied Perceptron</b>. It is capable of processing an arbitrary number of multi-modal inputs and demonstrates remarkable 3D perception capabilities, both within the two series of benchmarks we set up, i.e., fundamental 3D perception tasks and language-grounded tasks, and <b>in the wild</b>.
        </div>
      </div>
    </div>
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="100%">
            <source src="./static/videos/demo_editing_5.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div> -->
    <!--/ Abstract. -->

    <!-- Paper video. -->
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="./static/videos/teaser_gif.gif"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

      <!-- Pipeline -->
      <div class="columns is-centered has-text-centered">
      <h3 class="title is-4">Dataset Overview</h3>
      </div>
      <div class="content has-text-centered">
        <img width="75%" src="./static/images/teaser.png"">
      </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          EmbodiedScan provides a multi-modal, ego-centric 3D perception dataset with massive real-scanned data and rich annotations for indoor scenes. It benchmarks language-grounded holistic 3D scene understanding capabilities for real-world embodied agents.
        </div>
      </div>
    </div>
      <div class="content has-text-centered">
        <video id="replay-video"
               controls
               muted
               preload
               playsinline
               width="75%">
          <source src="./static/videos/dataset_overview.mp4"
                  type="video/mp4">
        <p>
          Dataset Overview.
        </p>
        </video>
      </div>
      <!--/ Pipeline -->
      </div>
    </div>
  

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

      <!-- Pipeline -->
      <div class="columns is-centered has-text-centered">
      <h3 class="title is-4">Framework Pipeline</h3>
      </div>
      <div class="content has-text-centered">
        <img width="75%" src="./static/images/framework.png"">
      </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          Embodied Perceptron accepts RGB-D sequence with any number of views along with texts as multi-modal input. It uses classical encoders to extract features for each modality and adopts dense and isomorphic sparse fusion with corresponding decoders for different predictions. The 3D features integrated with the text feature can be further used for language-grounded understanding.
        </div>
      </div>
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop">

      <h3 class="title is-4">SAM-Assisted Annotation</h3>
      <div class="content has-text-centered">
        <video id="replay-video"
               controls
               muted
               preload
               playsinline
               width="75%">
          <source src="./static/videos/annotation.mp4"
                  type="video/mp4">
        </video>
      </div>
      <!--/ Matting. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
  
      <!-- Multi-agent. -->
      <h3 class="title is-4">In-the-Wild Test</h3>
      <div class="content has-text-justified">
        <p>
          Trained with <span class="dnerf">EmbodiedScan</span>, our model shows remarkable 3D perception capabilities both in our established benchmarks and in the wild, even with a different RGB-D sensor in a different environment.
        </p>
      </div>
      <div class="content has-text-centered">
        <video id="replay-video"
               controls
               muted
               preload
               playsinline
               width="75%">
          <source src="./static/videos/in_the_wild.mp4"
                  type="video/mp4">
        </video>
      </div>
      <!--/ Multi-agent. -->
  

  
    </div>
  </section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wang2023embodiedscan,
  author={Wang, Tai and Mao, Xiaohan and Zhu, Chenming and Xu, Runsen and Lyu, Ruiyuan and Li, Peisen and Chen, Xiao and Zhang, Wenwei and Chen, Kai and Xue, Tianfan and Liu, Xihui and Lu, Cewu and Lin, Dahua and Pang, Jiangmiao},
  title={EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI},
  journal={Arxiv},
  year={2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/paper/EmbodiedScan.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/OpenRobotLab/EmbodiedScan" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template/">Academic Project Page Template</a>
            which was adopted from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
          </p>
          <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5x18u1vl8d4&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
